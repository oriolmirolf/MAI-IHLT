{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IHLT Lab 5\n",
        "\n",
        "Lab developed by:\n",
        "- Niklas Long Schiefelbein\n",
        "- Oriol Miró López-Feliu\n",
        "\n",
        "\n",
        "**Exercice description:**\n",
        "Given the following (lemma, category) pairs:\n",
        "```\n",
        "(’the’,’DT’), (’man’,’NN’), (’swim’,’VB’), (’with’, ’PR’), (’a’, ’DT’),\n",
        "(’girl’,’NN’), (’and’, ’CC’), (’a’, ’DT’), (’boy’, ’NN’), (’whilst’, ’PR’),\n",
        "(’the’, ’DT’), (’woman’, ’NN’), (’walk’, ’VB’)\n",
        "```\n",
        "1. For each pair, when possible, print their most frequent WordNet synset\n",
        "\n",
        "2. For each pair of words, when possible, print their corresponding least common subsumer (LCS) and their similarity value, using the following functions:\n",
        "  - Path Similarity\n",
        "  - Leacock-Chodorow Similarity\n",
        "  - Wu-Palmer Similarity\n",
        "  - Lin Similarity\n",
        "\n",
        "  Normalize similarity values when necessary. What similarity seems better?"
      ],
      "metadata": {
        "id": "SWkG87xpdZME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "8Nq_Is0UeBDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic\n",
        "import nltk\n",
        "\n",
        "# normalising\n",
        "import math\n",
        "\n",
        "# wordnet\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# for lin similarity (we need corpus)\n",
        "nltk.download('wordnet_ic')\n",
        "from nltk.corpus import wordnet_ic\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')"
      ],
      "metadata": {
        "id": "2DEdn_ceyQpQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b914d8-7347-41e1-d95b-a8ec4b0ece34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "nd3SLbvYd0cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in this case, fairly simple...\n",
        "\n",
        "# (lemma, category) pairs\n",
        "pairs = [('the', 'DT'), ('man', 'NN'), ('swim', 'VB'), ('with', 'PR'), ('a', 'DT'),\n",
        "         ('girl', 'NN'), ('and', 'CC'), ('a', 'DT'), ('boy', 'NN'), ('whilst', 'PR'),\n",
        "         ('the', 'DT'), ('woman', 'NN'), ('walk', 'VB')]"
      ],
      "metadata": {
        "id": "x2LJ8EhSd0lW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercices"
      ],
      "metadata": {
        "id": "lTWAAGM0l9HH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. For each pair, when possible, print their most frequent WordNet synset\n",
        "\n",
        "Note: I usually put functions in a separate \"useful functions section\", but in this case given they fully solve the exercise I deemed them more fit here."
      ],
      "metadata": {
        "id": "tXbrG5LZl-dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we only consider nouns (NN) and verbs (VB), since WordNet does not support\n",
        "#   determiners (DT), prepositions (PR) and conjunctions (CC) (see https://wordnet.princeton.edu/frequently-asked-questions)\n",
        "\n",
        "def get_most_frequent_synset(lemma, category):\n",
        "    if category == 'NN':\n",
        "        synsets = wn.synsets(lemma, pos=wn.NOUN)\n",
        "    elif category == 'VB':\n",
        "        synsets = wn.synsets(lemma, pos=wn.VERB)\n",
        "    else:\n",
        "        return None  # DR, PR, CC\n",
        "\n",
        "    return synsets[0] if synsets else None # most common == first!\n"
      ],
      "metadata": {
        "id": "z5qYE0Eul9_1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop calc\n",
        "synsets = []\n",
        "for lemma, category in pairs:\n",
        "    synset = get_most_frequent_synset(lemma, category)\n",
        "    if synset:\n",
        "        print(f\"Most frequent synset for {lemma} ({category}): {synset.name()}\")\n",
        "        synsets.append(synset) # we save them for latter calculations (ex 2)\n",
        "    else:\n",
        "        print(f\"No synset found for {lemma} ({category})\") # :("
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMjQGaC7mNUd",
        "outputId": "b069efd1-dd1d-47e9-b39a-cc403db12bc4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No synset found for the (DT)\n",
            "Most frequent synset for man (NN): man.n.01\n",
            "Most frequent synset for swim (VB): swim.v.01\n",
            "No synset found for with (PR)\n",
            "No synset found for a (DT)\n",
            "Most frequent synset for girl (NN): girl.n.01\n",
            "No synset found for and (CC)\n",
            "No synset found for a (DT)\n",
            "Most frequent synset for boy (NN): male_child.n.01\n",
            "No synset found for whilst (PR)\n",
            "No synset found for the (DT)\n",
            "Most frequent synset for woman (NN): woman.n.01\n",
            "Most frequent synset for walk (VB): walk.v.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 2\n",
        "\n",
        "For each pair of words, when possible, print their corresponding least common subsumer (LCS) and their similarity value, using the following functions:\n",
        "  - Path Similarity\n",
        "  - Leacock-Chodorow Similarity\n",
        "  - Wu-Palmer Similarity\n",
        "  -Lin Similarity\n",
        "\n",
        "  Normalize similarity values when necessary. What similarity seems better?"
      ],
      "metadata": {
        "id": "_XmAFf31oEOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for normalization in Leacock-Chodorow Similarity\n",
        "\n",
        "max_depth_nouns = max(len(path) for synset in wn.all_synsets('n') for path in synset.hypernym_paths())\n",
        "max_depth_verbs = max(len(path) for synset in wn.all_synsets('v') for path in synset.hypernym_paths())\n",
        "\n",
        "max_lch_noun  = -math.log(1 / (2 * max_depth_nouns))\n",
        "max_lch_verb  = -math.log(1 / (2 * max_depth_verbs))\n",
        "\n",
        "# debug... (left as it is interesting to see!)\n",
        "print(f\"Max depth nouns: {max_depth_nouns}\")\n",
        "print(f\"Max depth verbs: {max_depth_verbs}\")\n",
        "print(f\"Max LCH noun: {max_lch_noun}\")\n",
        "print(f\"Max LCH verb: {max_lch_verb}\")"
      ],
      "metadata": {
        "id": "3albwM5EQPOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb45367-af47-43cc-cb4e-aeb93b123cdc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max depth nouns: 20\n",
            "Max depth verbs: 13\n",
            "Max LCH noun: 3.6888794541139363\n",
            "Max LCH verb: 3.258096538021482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we decided to compute similarity between every pair of words, despite different POS tags, for completeness, becasuse Path Similarity does yield a metric (despite bad)\n",
        "\n",
        "def compute_similarities(synset1, synset2):\n",
        "    print(f\"\\n------------------------\\n\\nComparing {synset1.name()} and {synset2.name()}:\")\n",
        "\n",
        "    # basic path similarity\n",
        "    path_sim = synset1.path_similarity(synset2)\n",
        "    print(f\"Path Similarity: {path_sim:.3f}\")\n",
        "\n",
        "    # the rest of similarities require same POS\n",
        "\n",
        "    if synset1.pos() != synset2.pos():\n",
        "        print(f\"Leacock-Chodorow Similarity not computable\")\n",
        "        print(f\"Wu-Palmer Similarity not computable\")\n",
        "        print(f\"Lin Similarity not computable\")\n",
        "        print(\"No LCS found\")\n",
        "        return\n",
        "\n",
        "    # => implicit else\n",
        "\n",
        "    if synset1.pos() == 'v':\n",
        "        max_lch = max_lch_verb\n",
        "    else:\n",
        "        max_lch = max_lch_noun\n",
        "\n",
        "    # lch similarity\n",
        "    lch_sim = synset1.lch_similarity(synset2)\n",
        "    norm_lch_sim = lch_sim / max_lch\n",
        "\n",
        "    # wp similarity\n",
        "    wup_sim = synset1.wup_similarity(synset2)\n",
        "\n",
        "    # lin similarity\n",
        "    lin_sim = synset1.lin_similarity(synset2, brown_ic)\n",
        "\n",
        "    # least common subsumer for both synsets\n",
        "    lcs = synset1.lowest_common_hypernyms(synset2)\n",
        "\n",
        "    # we print\n",
        "    # print(f\"Leacock-Chodorow Similarity: {lch_sim}\") # uncomment for debug\n",
        "    print(f\"Normalized Leacock-Chodorow Similarity: {norm_lch_sim:.3f}\")\n",
        "    print(f\"Wu-Palmer Similarity: {wup_sim:.3f}\")\n",
        "    print(f\"Lin Similarity: {lin_sim:.3f}\")\n",
        "    print(f\"Least Common Subsumer (LCS): {lcs[0].name()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "M_KeGWNVoJUi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all combination of pairs (non repeated)\n",
        "for i in range(len(synsets)):\n",
        "    for j in range(i+1, len(synsets)):\n",
        "        # if synsets[i].pos() == synsets[j].pos(): if one wants to only compare the same POS\n",
        "        compute_similarities(synsets[i], synsets[j])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wtwvz5-8qRVM",
        "outputId": "89d5bbc4-bcb8-4466-9746-03772949c89d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------\n",
            "\n",
            "Comparing man.n.01 and swim.v.01:\n",
            "Path Similarity: 0.100\n",
            "Leacock-Chodorow Similarity not computable\n",
            "Wu-Palmer Similarity not computable\n",
            "Lin Similarity not computable\n",
            "No LCS found\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing man.n.01 and girl.n.01:\n",
            "Path Similarity: 0.250\n",
            "Normalized Leacock-Chodorow Similarity: 0.610\n",
            "Wu-Palmer Similarity: 0.632\n",
            "Lin Similarity: 0.714\n",
            "Least Common Subsumer (LCS): adult.n.01\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing man.n.01 and male_child.n.01:\n",
            "Path Similarity: 0.333\n",
            "Normalized Leacock-Chodorow Similarity: 0.688\n",
            "Wu-Palmer Similarity: 0.667\n",
            "Lin Similarity: 0.729\n",
            "Least Common Subsumer (LCS): male.n.02\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing man.n.01 and woman.n.01:\n",
            "Path Similarity: 0.333\n",
            "Normalized Leacock-Chodorow Similarity: 0.688\n",
            "Wu-Palmer Similarity: 0.667\n",
            "Lin Similarity: 0.787\n",
            "Least Common Subsumer (LCS): adult.n.01\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing man.n.01 and walk.v.01:\n",
            "Path Similarity: 0.100\n",
            "Leacock-Chodorow Similarity not computable\n",
            "Wu-Palmer Similarity not computable\n",
            "Lin Similarity not computable\n",
            "No LCS found\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing swim.v.01 and girl.n.01:\n",
            "Path Similarity: 0.091\n",
            "Leacock-Chodorow Similarity not computable\n",
            "Wu-Palmer Similarity not computable\n",
            "Lin Similarity not computable\n",
            "No LCS found\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing swim.v.01 and male_child.n.01:\n",
            "Path Similarity: 0.100\n",
            "Leacock-Chodorow Similarity not computable\n",
            "Wu-Palmer Similarity not computable\n",
            "Lin Similarity not computable\n",
            "No LCS found\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing swim.v.01 and woman.n.01:\n",
            "Path Similarity: 0.100\n",
            "Leacock-Chodorow Similarity not computable\n",
            "Wu-Palmer Similarity not computable\n",
            "Lin Similarity not computable\n",
            "No LCS found\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing swim.v.01 and walk.v.01:\n",
            "Path Similarity: 0.333\n",
            "Normalized Leacock-Chodorow Similarity: 0.663\n",
            "Wu-Palmer Similarity: 0.333\n",
            "Lin Similarity: 0.491\n",
            "Least Common Subsumer (LCS): travel.v.01\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing girl.n.01 and male_child.n.01:\n",
            "Path Similarity: 0.167\n",
            "Normalized Leacock-Chodorow Similarity: 0.500\n",
            "Wu-Palmer Similarity: 0.632\n",
            "Lin Similarity: 0.293\n",
            "Least Common Subsumer (LCS): person.n.01\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing girl.n.01 and woman.n.01:\n",
            "Path Similarity: 0.500\n",
            "Normalized Leacock-Chodorow Similarity: 0.798\n",
            "Wu-Palmer Similarity: 0.632\n",
            "Lin Similarity: 0.907\n",
            "Least Common Subsumer (LCS): woman.n.01\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing girl.n.01 and walk.v.01:\n",
            "Path Similarity: 0.091\n",
            "Leacock-Chodorow Similarity not computable\n",
            "Wu-Palmer Similarity not computable\n",
            "Lin Similarity not computable\n",
            "No LCS found\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing male_child.n.01 and woman.n.01:\n",
            "Path Similarity: 0.200\n",
            "Normalized Leacock-Chodorow Similarity: 0.550\n",
            "Wu-Palmer Similarity: 0.667\n",
            "Lin Similarity: 0.318\n",
            "Least Common Subsumer (LCS): person.n.01\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing male_child.n.01 and walk.v.01:\n",
            "Path Similarity: 0.100\n",
            "Leacock-Chodorow Similarity not computable\n",
            "Wu-Palmer Similarity not computable\n",
            "Lin Similarity not computable\n",
            "No LCS found\n",
            "\n",
            "------------------------\n",
            "\n",
            "Comparing woman.n.01 and walk.v.01:\n",
            "Path Similarity: 0.100\n",
            "Leacock-Chodorow Similarity not computable\n",
            "Wu-Palmer Similarity not computable\n",
            "Lin Similarity not computable\n",
            "No LCS found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What similarity seems better?**\n",
        "\n",
        "Comparing metrics in NLP is always tough, as different metrics can be better in different contexts. One way could be comparing each metric against the similarity computed through word embeddings, as they have been proven robust (and are currently the state-of-the-art). Nevertheless, in this task, we will evaluate metrics against our \"human evaluations\" (themselves full of biases!), comparing what metric captures our intuitions best across cases.\n",
        "\n",
        "- **Path Similarity:** It offers an advantage as it is computable across POS, giving us an \"aproximate idea\" of similarity, but for any other case it fails to capture similarity as well as other metrics; for example, it scores with a mere 0.33 the similarity between \"man\" and \"male_child\".\n",
        "\n",
        "- **Leacock-Chodorow Similarity:** Is closer to our intuition; it (in our opinion) correctly captures the similarity between \"man\" and \"woman\" being the same as between \"man\" and \"male_child\", as we believe both only differ in one dimension (gender and age, respectively). However, such \"dimension difference\" argument does not always apply, as the similarity between \"girl\" and \"male_child\" is lower than we would expect (0.5); in cases such as this one, it fails to capture the meaning of the words.\n",
        "\n",
        "- **Wu-Palmer Similarity:** Regarding the previous example, it correctly computes \"girl\" and \"male_child\" to be similar, more so than any other metric; nevertheless, we believe it sometimes underestimates similarities, for example between \"swim\" and \"walk\" (0.333) or between \"girl\" and \"woman\" (0.632).\n",
        "\n",
        "- **Lin Similarity:** This metric is hard to evaluate as it also depends on the training corpus, so our analysis might be influenced by our choice (https://www.nltk.org/howto/wordnet.html). We found Lin Similarity to be the most fine-grained, as all values computed are different. In many cases the similarities align well with our intuitions (e.g. \"man\" and \"male_child\" being more similar than \"man\" and \"girl\"), altough in others it seems to underestimate similarity (e.g. \"girl\" and \"male_child\" being much less similar than \"man\" and \"woman\", despite the difference in both mainly being gender). This unpredictability poses a major drawback.\n",
        "\n",
        "In conclusion, no single metric is perfect for capturing our intuitive understanding of word similarity. However, we believe **Wu-Palmer Similarity** is the most balanced among those evaluated (despite not by much). It generally aligns well with our intuitions, capturing similarities across a range of cases without being as unpredictable as some of the other metrics. We must however take into account the low number of samples evaluated, meaning this conclusion does not carry much weight.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XmQrJvn4ch_8"
      }
    }
  ]
}